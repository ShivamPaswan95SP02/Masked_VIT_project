{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24786b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621ad7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_batch_and_samples(data_loader, batch_idx, sample_indices):\n",
    "    batches = [batch for batch_idx_, batch in enumerate(data_loader) if batch_idx_ == batch_idx]\n",
    "    if not batches:\n",
    "        raise ValueError(\"Batch index out of range\")\n",
    "    batch = batches[0]\n",
    "    samples = batch[0][sample_indices]\n",
    "    return samples\n",
    "\n",
    "def apply_patching(images, patch_size=4):\n",
    "    batch_size, channels, height, width = images.shape\n",
    "    patches = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    patches = patches.contiguous().view(batch_size, channels, -1, patch_size, patch_size)\n",
    "    patches = patches.permute(0, 2, 1, 3, 4)\n",
    "    patches = patches.contiguous().view(batch_size * patches.size(1), channels, patch_size, patch_size)\n",
    "    return patches\n",
    "\n",
    "def apply_masking(patches, mask_percent=0.75):\n",
    "    batch_size = patches.shape[0]\n",
    "    mask = torch.rand(batch_size) < mask_percent\n",
    "    masked_patches = patches.clone()\n",
    "    masked_patches[mask] = 0\n",
    "    return masked_patches, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501694d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, embedding_dim=64):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.patch_embedding = nn.Linear(patch_size * patch_size, embedding_dim)\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(28 // patch_size * 28 // patch_size, embedding_dim))\n",
    "\n",
    "    def forward(self, patches):\n",
    "        batch_size, _, h, w = patches.shape\n",
    "        patches = patches.view(batch_size, -1, h * w)\n",
    "        embedded_patches = self.patch_embedding(patches)\n",
    "        embedded_patches += self.positional_embedding.unsqueeze(0)\n",
    "        return embedded_patches\n",
    "\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=64):\n",
    "        super(MaskedAutoencoder, self).__init__()\n",
    "        self.patch_embedding = PatchEmbedding(embedding_dim=embedding_dim)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4),\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4),\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4)\n",
    "        )\n",
    "        self.reconstruction = nn.Linear(embedding_dim, 4 * 4)\n",
    "\n",
    "    def forward(self, patches, mask):\n",
    "        embedded_patches = self.patch_embedding(patches)\n",
    "        encoded_patches = self.encoder(embedded_patches)\n",
    "        decoded_patches = self.decoder(encoded_patches)\n",
    "        reconstructed_patches = self.reconstruction(decoded_patches)\n",
    "        return reconstructed_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c5130b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "torch.__version__\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cdb9395",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/content/MICCAI-Educational-Challenge-2024/train'\n",
    "test_dir = '/content/MICCAI-Educational-Challenge-2024/test'\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "def create_dataloaders(\n",
    " train_dir: str,\n",
    " test_dir: str,\n",
    " transform: transforms.Compose,\n",
    " batch_size: int,\n",
    " num_workers: int=NUM_WORKERS\n",
    " ):\n",
    " # Use ImageFolder to create dataset(s)\n",
    " train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    " test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    " # Get class names\n",
    " class_names = train_data.classes\n",
    " # Turn images into train and test data loaders\n",
    " train_dataloader = DataLoader(\n",
    " train_data,\n",
    " batch_size=batch_size,\n",
    " shuffle=True,\n",
    " num_workers=num_workers,\n",
    " pin_memory=True,\n",
    " )\n",
    " test_dataloader = DataLoader(\n",
    " test_data,\n",
    " batch_size=batch_size,\n",
    " shuffle=False,\n",
    " num_workers=num_workers,\n",
    " pin_memory=True,\n",
    " )\n",
    " return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5f9e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually create Tranform:Compose(\n",
      "    Resize(size=(240, 240), interpolation=bilinear, max_size=None, antialias=None)\n",
      "    ToTensor()\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomVerticalFlip(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 240 \n",
    "\n",
    "manual_tranforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE , IMG_SIZE)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "\n",
    "])\n",
    "print(f\"Manually create Tranform:{manual_tranforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73bdc0c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "rotate() missing 1 required positional argument: 'angle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Apply masked transformation\u001b[39;00m\n\u001b[0;32m     42\u001b[0m angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m  \u001b[38;5;66;03m# Rotation angle\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m transformed_image \u001b[38;5;241m=\u001b[39m \u001b[43mmasked_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Plot the original image, mask, and transformed image\u001b[39;00m\n\u001b[0;32m     46\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m, in \u001b[0;36mmasked_transform\u001b[1;34m(image, mask, angle)\u001b[0m\n\u001b[0;32m     11\u001b[0m mask_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(mask)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Apply rotation to the entire image\u001b[39;00m\n\u001b[0;32m     14\u001b[0m rotation_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     15\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToPILImage(),\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     17\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[0;32m     18\u001b[0m ])\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Create a transformed version of the image\u001b[39;00m\n\u001b[0;32m     21\u001b[0m transformed_image \u001b[38;5;241m=\u001b[39m rotation_transform(image)\n",
      "\u001b[1;31mTypeError\u001b[0m: rotate() missing 1 required positional argument: 'angle'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define a masked transformation function\n",
    "def masked_transform(image, mask, angle):\n",
    "    # Convert image and mask to numpy arrays\n",
    "    image_np = np.array(image)\n",
    "    mask_np = np.array(mask)\n",
    "\n",
    "    # Apply rotation to the entire image\n",
    "    rotation_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.functional.rotate(angle),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Create a transformed version of the image\n",
    "    transformed_image = rotation_transform(image)\n",
    "\n",
    "    # Convert the transformed image back to a numpy array\n",
    "    transformed_image_np = transformed_image.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Apply the mask to combine the original and transformed images\n",
    "    masked_image_np = image_np * (1 - mask_np) + transformed_image_np * mask_np\n",
    "\n",
    "    return torch.from_numpy(masked_image_np).permute(2, 0, 1)\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Select an image from the dataset\n",
    "image, label = mnist_dataset[0]\n",
    "\n",
    "# Create a simple mask (e.g., mask the upper half of the image)\n",
    "mask = torch.zeros_like(image)\n",
    "mask[0, :14, :] = 1  # Mask the upper half\n",
    "\n",
    "# Apply masked transformation\n",
    "angle = 30  # Rotation angle\n",
    "transformed_image = masked_transform(image, mask, angle)\n",
    "\n",
    "# Plot the original image, mask, and transformed image\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].imshow(image.squeeze(), cmap='gray')\n",
    "axs[0].set_title('Original Image')\n",
    "axs[1].imshow(mask.squeeze(), cmap='gray')\n",
    "axs[1].set_title('Mask')\n",
    "axs[2].imshow(transformed_image.squeeze(), cmap='gray')\n",
    "axs[2].set_title('Transformed Image')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainee_sp00",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
